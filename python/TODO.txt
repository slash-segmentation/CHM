Current major issues to address:
	TRAIN:	using non-compat Gabor and SIFT filters causes issues
	TEST:	tiling has some seem effects even though it shouldn't
			I have done a lot to reduce this (originally I was getting things -/+200, now just -/+32)
	FILTER: SIFT is performing sub-optimally (both in time and accuracy)
	FILTER: higher levels have much-reduced accuracy compared to MATLAB
			even inten - thus does MyDownSample have issues?
	FILTER: correlate_xy (used by Edge, Frangi, and SIFT) currently can't do multithreaded in-place

Current planned optimizations:
	TEST:	when doing a tile-subset only copy the surrounding region into shared memory
			also, only downsample that region
	FILTER:	optimize SIFT more
	UTIL:	optimize/multi-thread compress, numpy.pad, MyMaxPooling, and/or im2double
	TRAIN:	multi-thread learning?
	TRAIN:	save histogram?
	TRAIN:	subsample?
	FILTER/TEST/TRAIN: see if any algorithms would still benefit from being made into CUDA vareties

Minor optimizations that could be done:
	TEST:	give each process a different number of threads when nthreads is fractional
	TRAIN:	use ntasks in addition to nthreads
	FILTER:	find a way to increase performance of pixel-at-once filters (Haar, HOG, SIFT)
			Haar can do X and Y separately with just a little extra shared computation
	FILTER:	Haar cc_cmp_II in place?
	FILTER:	Edge one less intermediate?
