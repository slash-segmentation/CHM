Current major issues to address:
	TRAIN:	using non-compat Gabor and SIFT filters causes issues
	TEST:	deadlock on leaving first parallel block when re-running CHM-test
	FILTER: SIFT is performing sub-optimally (both in time and accuracy)
	FILTER: SIFT has a potential divide-by-zero
    FILTER: SIFT has some very very minor seem effects, possibly due ot the MATLAB compat mode
	FILTER: higher levels have much-reduced accuracy compared to MATLAB
			even inten - thus does MyDownSample have issues?
	FILTER: correlate_xy (used by Edge, Frangi, and eventually SIFT) currently can't do multithreaded in-place
			it is possible that pyfftw would be better than correlate_xy anyways

Current planned optimizations:
	TEST:	when doing a tile-subset only copy the surrounding region into shared memory
			also, only downsample that region
	FILTER:	optimize SIFT more
	UTIL:	optimize/multi-thread compress and/or im2double
	TRAIN:	multi-thread ldnn:init_weights, ldnn:gradient_descent, ldnn:gradient_descent_dropout?
	TRAIN:	save histogram?
	TRAIN:	subsample?
	FILTER/TEST/TRAIN: see if any algorithms would still benefit from being made into CUDA vareties

Minor optimizations that could be done:
	TEST:	give each process a different number of threads when nthreads is fractional
	TRAIN:	use ntasks in addition to nthreads
	FILTER:	find a way to increase performance of pixel-at-once filters (Haar, HOG, SIFT)
			Haar can do X and Y separately with just a little extra shared computation
	FILTER:	Haar cc_cmp_II in place?
	FILTER:	Edge one less intermediate?
